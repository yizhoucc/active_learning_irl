{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up device ---\n",
      "Using MPS device (Apple Silicon GPU)\n",
      "Loading data from: data/ppo_baseline_0331_5cost_data\n",
      "Loaded 125000 samples.\n",
      "Processing target variables (y_data)...\n",
      "  Target shape: torch.Size([125000, 3])\n",
      "  Targets normalized by max value per column.\n",
      "Processing input variables (x_data)...\n",
      "  Input feature size: 5\n",
      "  Input features normalized (StandardScaler).\n",
      "  Padded data shape: torch.Size([125000, 500, 5])\n",
      "Creating datasets and dataloaders...\n",
      "  Train samples: 100000\n",
      "  Validation samples: 12500\n",
      "  Test samples: 12500\n",
      "Initializing model...\n",
      "GRUNet(\n",
      "  (gru): GRU(5, 64, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Total parameters: 38787\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import sys # Import sys for sys.exit()\n",
    "import time # For timing epochs\n",
    "\n",
    "# --- Configuration ---\n",
    "note = 'data'\n",
    "agent_name = 'ppo_baseline_0331_5cost'\n",
    "data_path = f'data/{agent_name}_{note}'\n",
    "model_save_path = f'data/{agent_name}_{note}_mps.pt' # Added suffix for clarity\n",
    "batch_size = 64 # <<< Consider increasing batch size for GPU utilization >>>\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "hidden_size = 64 # <<< Consider increasing model size for GPU >>>\n",
    "num_layers = 2\n",
    "gradient_clip_value = 5.0\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "# --- Device Setup (MPS Acceleration) ---\n",
    "print(\"--- Setting up device ---\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "# Optional: Check if MPS fallback is needed (rarely necessary now)\n",
    "# print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "# print(f\"MPS fallback enabled: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "try:\n",
    "    with open(data_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        if not isinstance(loaded_data, (list, tuple)) or len(loaded_data) != 2:\n",
    "             raise TypeError(\"Pickle file should contain a list or tuple of two elements: x_data and ys\")\n",
    "        x_data_raw, ys_raw = loaded_data\n",
    "        if not isinstance(x_data_raw, list) or not isinstance(ys_raw, list):\n",
    "             raise TypeError(\"Both elements in the loaded pickle file should be lists.\")\n",
    "        print(f\"Loaded {len(x_data_raw)} samples.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_path}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing pickle file: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# 1. Target Variable Processing\n",
    "print(\"Processing target variables (y_data)...\")\n",
    "try:\n",
    "    y_data = [torch.tensor(y, dtype=torch.float32).view(-1) for y in ys_raw]\n",
    "    y_data = torch.stack(y_data)\n",
    "    target_indices = [3, 9, 11]\n",
    "    y_data = y_data[:, target_indices]\n",
    "    output_size = y_data.shape[1]\n",
    "    print(f\"  Target shape: {y_data.shape}\")\n",
    "\n",
    "    y_max_vals, _ = torch.max(y_data, dim=0, keepdim=True)\n",
    "    y_max_vals[y_max_vals == 0] = 1.0\n",
    "    y_data = y_data / y_max_vals\n",
    "    print(\"  Targets normalized by max value per column.\")\n",
    "\n",
    "except IndexError:\n",
    "     print(f\"Error: Target indices {target_indices} out of bounds for loaded y data.\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "     print(f\"Error processing y_data: {e}\")\n",
    "     sys.exit(1)\n",
    "\n",
    "# 2. Input Variable Processing (Normalization & Padding)\n",
    "print(\"Processing input variables (x_data)...\")\n",
    "x_data_tensors = [torch.tensor(x, dtype=torch.float32) for x in x_data_raw]\n",
    "\n",
    "if not x_data_tensors:\n",
    "    print(\"Error: No input data loaded.\")\n",
    "    sys.exit(1)\n",
    "input_size = x_data_tensors[0].shape[1]\n",
    "if any(t.ndim <= 1 or t.shape[1] != input_size for t in x_data_tensors):\n",
    "     print(\"Error: Input features have inconsistent dimensions or are not 2D across samples.\")\n",
    "     for i, t in enumerate(x_data_tensors):\n",
    "         if t.ndim <= 1 or t.shape[1] != input_size: print(f\"  Problematic sample index: {i}, shape: {t.shape}\")\n",
    "     sys.exit(1)\n",
    "print(f\"  Input feature size: {input_size}\")\n",
    "\n",
    "try:\n",
    "    all_x_concatenated = torch.cat(x_data_tensors, dim=0).numpy()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_x_concatenated)\n",
    "    x_data_normalized_tensors = [torch.tensor(scaler.transform(x.numpy()), dtype=torch.float32) for x in x_data_tensors]\n",
    "    print(\"  Input features normalized (StandardScaler).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during input normalization: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "padded_data = pad_sequence(x_data_normalized_tensors, batch_first=True, padding_value=0.0)\n",
    "print(f\"  Padded data shape: {padded_data.shape}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "num_samples = len(padded_data)\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "train_end_idx = int(num_samples * train_split)\n",
    "val_end_idx = train_end_idx + int(num_samples * val_split)\n",
    "if val_end_idx == train_end_idx and val_end_idx < num_samples: val_end_idx += 1\n",
    "elif val_end_idx >= num_samples: val_end_idx = train_end_idx\n",
    "\n",
    "train_indices = indices[:train_end_idx]\n",
    "val_indices = indices[train_end_idx:val_end_idx]\n",
    "test_indices = indices[val_end_idx:]\n",
    "\n",
    "print(f\"  Train samples: {len(train_indices)}\")\n",
    "print(f\"  Validation samples: {len(val_indices)}\")\n",
    "print(f\"  Test samples: {len(test_indices)}\")\n",
    "\n",
    "if len(train_indices) == 0 or len(test_indices) == 0: print(\"Warning: Training or test set is empty.\")\n",
    "if len(val_indices) == 0: print(\"Warning: Validation set is empty.\")\n",
    "\n",
    "train_dataset = TensorDataset(padded_data[train_indices], y_data[train_indices])\n",
    "val_dataset = TensorDataset(padded_data[val_indices], y_data[val_indices]) if len(val_indices) > 0 else None\n",
    "test_dataset = TensorDataset(padded_data[test_indices], y_data[test_indices])\n",
    "\n",
    "# Use pin_memory=True if using GPU for potentially faster data transfer\n",
    "pin_memory_flag = True if device != torch.device(\"cpu\") else False\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory_flag)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=pin_memory_flag) if val_dataset else None\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=pin_memory_flag)\n",
    "\n",
    "# --- Define Model ---\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        out, h_final = self.gru(x, h)\n",
    "        last_time_step_out = out[:, -1, :]\n",
    "        out = self.fc(last_time_step_out)\n",
    "        return out, h_final\n",
    "\n",
    "# --- Initialize Model, Loss, Optimizer ---\n",
    "print(\"Initializing model...\")\n",
    "model = GRUNet(input_size=input_size, hidden_size=hidden_size,\n",
    "               num_layers=num_layers, output_size=output_size).to(device) # Move model to device\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Move batch data to the device\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred, _ = model(x_batch, None)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), gradient_clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_epoch_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    all_val_outputs = []\n",
    "    all_val_targets = []\n",
    "\n",
    "    if val_loader:\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                # Move batch data to the device\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs, _ = model(x_batch, None)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                epoch_val_loss += loss.item()\n",
    "                all_val_outputs.append(outputs.cpu()) # Collect on CPU\n",
    "                all_val_targets.append(y_batch.cpu()) # Collect on CPU\n",
    "\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_epoch_val_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Val Loss = {avg_epoch_val_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "\n",
    "        # Optional plotting (consider doing it less frequently)\n",
    "        # if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        #    # ... (plotting code as before) ...\n",
    "\n",
    "    else: # No validation set\n",
    "         epoch_time = time.time() - epoch_start_time\n",
    "         print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "         val_losses.append(None)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# --- Testing ---\n",
    "print(\"\\n--- Starting Testing ---\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_test_outputs = []\n",
    "all_test_targets = []\n",
    "\n",
    "if not test_loader:\n",
    "     print(\"Warning: No test data loaded. Skipping testing.\")\n",
    "else:\n",
    "    test_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Move batch data to the device\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs, _ = model(x_batch, None)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            all_test_outputs.append(outputs.cpu()) # Collect on CPU\n",
    "            all_test_targets.append(y_batch.cpu()) # Collect on CPU\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    if len(test_loader) > 0:\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Final Test Loss (MSE): {avg_test_loss:.5f}, Test Time = {test_time:.2f}s\")\n",
    "\n",
    "        all_test_outputs = torch.cat(all_test_outputs, dim=0).numpy()\n",
    "        all_test_targets = torch.cat(all_test_targets, dim=0).numpy()\n",
    "\n",
    "        # --- Plot Test Results ---\n",
    "        print(\"Plotting test results...\")\n",
    "        num_outputs_to_plot = all_test_outputs.shape[1]\n",
    "        fig_test, axes_test = plt.subplots(1, num_outputs_to_plot, figsize=(6 * num_outputs_to_plot, 5.5), squeeze=False)\n",
    "\n",
    "        for i in range(num_outputs_to_plot):\n",
    "            ax = axes_test[0, i]\n",
    "            outputs_i = all_test_outputs[:, i]\n",
    "            targets_i = all_test_targets[:, i]\n",
    "            ax.scatter(outputs_i, targets_i, alpha=0.2)\n",
    "            min_val = min(outputs_i.min() if len(outputs_i) > 0 else 0, targets_i.min() if len(targets_i) > 0 else 0)\n",
    "            max_val = max(outputs_i.max() if len(outputs_i) > 0 else 1, targets_i.max() if len(targets_i) > 0 else 1)\n",
    "            padding = (max_val - min_val) * 0.05\n",
    "            ax.plot([min_val - padding, max_val + padding], [min_val - padding, max_val + padding], 'r--', label='y=x')\n",
    "            ax.set_title(f'Test Set - Output {i+1} (Index {target_indices[i]})')\n",
    "            ax.set_xlabel(\"Predicted Value\")\n",
    "            ax.set_ylabel(\"Actual Value\")\n",
    "            ax.axis('equal')\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Test loader was empty. No test results to plot.\")\n",
    "\n",
    "\n",
    "# --- Plot Training/Validation Loss Curve ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "valid_epochs = [i+1 for i, l in enumerate(val_losses) if l is not None]\n",
    "valid_val_losses = [l for l in val_losses if l is not None]\n",
    "if valid_val_losses:\n",
    "    plt.plot(valid_epochs, valid_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Save Model (Optional) ---\n",
    "# print(f\"\\nSaving model to: {model_save_path}\")\n",
    "# torch.save({\n",
    "#     'epoch': num_epochs,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': avg_test_loss if test_loader and len(test_loader) > 0 else None,\n",
    "#     'input_size': input_size,\n",
    "#     'hidden_size': hidden_size,\n",
    "#     'num_layers': num_layers,\n",
    "#     'output_size': output_size,\n",
    "#     'target_indices': target_indices,\n",
    "#     'scaler_mean': scaler.mean_,\n",
    "#     'scaler_scale': scaler.scale_,\n",
    "#     'y_max_vals': y_max_vals.cpu().numpy()\n",
    "# }, model_save_path)\n",
    "# print(\"Model saved.\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Model (Optional) ---\n",
    "print(f\"\\nSaving model to: {model_save_path}\")\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_test_loss if test_loader and len(test_loader) > 0 else None, # Save final test loss if available\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "    'output_size': output_size,\n",
    "    'target_indices': target_indices,\n",
    "    'scaler_mean': scaler.mean_, # Save scaler parameters\n",
    "    'scaler_scale': scaler.scale_,\n",
    "    'y_max_vals': y_max_vals.cpu().numpy() # Save target normalization factors\n",
    "}, model_save_path)\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# --- Notify (Optional) ---\n",
    "try:\n",
    "    notify()\n",
    "except NameError:\n",
    "    pass # Ignore if notify function isn't defined\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
