{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up device ---\n",
      "Using MPS device (Apple Silicon GPU)\n",
      "Loading data from: data/ppo_baseline_0331_5cost_data\n",
      "Loaded 125000 samples.\n",
      "Processing target variables (y_data)...\n",
      "  Target shape: torch.Size([125000, 3])\n",
      "  Targets normalized by max value per column.\n",
      "Processing input variables (x_data)...\n",
      "  Input feature size: 5\n",
      "  Input features normalized (StandardScaler).\n",
      "  Padded data shape: torch.Size([125000, 500, 5])\n",
      "Creating datasets and dataloaders...\n",
      "  Train samples: 100000\n",
      "  Validation samples: 12500\n",
      "  Test samples: 12500\n",
      "Initializing model...\n",
      "GRUNet(\n",
      "  (gru): GRU(5, 64, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Total parameters: 38787\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import sys # Import sys for sys.exit()\n",
    "import time # For timing epochs\n",
    "\n",
    "# --- Configuration ---\n",
    "note = 'data'\n",
    "agent_name = 'ppo_baseline_0331_5cost'\n",
    "data_path = f'data/{agent_name}_{note}'\n",
    "model_save_path = f'data/{agent_name}_{note}_mps.pt' # Added suffix for clarity\n",
    "batch_size = 64 # <<< Consider increasing batch size for GPU utilization >>>\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "hidden_size = 64 # <<< Consider increasing model size for GPU >>>\n",
    "num_layers = 2\n",
    "gradient_clip_value = 5.0\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "# --- Device Setup (MPS Acceleration) ---\n",
    "print(\"--- Setting up device ---\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "# Optional: Check if MPS fallback is needed (rarely necessary now)\n",
    "# print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "# print(f\"MPS fallback enabled: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "try:\n",
    "    with open(data_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        if not isinstance(loaded_data, (list, tuple)) or len(loaded_data) != 2:\n",
    "             raise TypeError(\"Pickle file should contain a list or tuple of two elements: x_data and ys\")\n",
    "        x_data_raw, ys_raw = loaded_data\n",
    "        if not isinstance(x_data_raw, list) or not isinstance(ys_raw, list):\n",
    "             raise TypeError(\"Both elements in the loaded pickle file should be lists.\")\n",
    "        print(f\"Loaded {len(x_data_raw)} samples.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_path}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing pickle file: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# 1. Target Variable Processing\n",
    "print(\"Processing target variables (y_data)...\")\n",
    "try:\n",
    "    y_data = [torch.tensor(y, dtype=torch.float32).view(-1) for y in ys_raw]\n",
    "    y_data = torch.stack(y_data)\n",
    "    target_indices = [3, 9, 11]\n",
    "    y_data = y_data[:, target_indices]\n",
    "    output_size = y_data.shape[1]\n",
    "    print(f\"  Target shape: {y_data.shape}\")\n",
    "\n",
    "    y_max_vals, _ = torch.max(y_data, dim=0, keepdim=True)\n",
    "    y_max_vals[y_max_vals == 0] = 1.0\n",
    "    y_data = y_data / y_max_vals\n",
    "    print(\"  Targets normalized by max value per column.\")\n",
    "\n",
    "except IndexError:\n",
    "     print(f\"Error: Target indices {target_indices} out of bounds for loaded y data.\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "     print(f\"Error processing y_data: {e}\")\n",
    "     sys.exit(1)\n",
    "\n",
    "# 2. Input Variable Processing (Normalization & Padding)\n",
    "print(\"Processing input variables (x_data)...\")\n",
    "x_data_tensors = [torch.tensor(x, dtype=torch.float32) for x in x_data_raw]\n",
    "\n",
    "if not x_data_tensors:\n",
    "    print(\"Error: No input data loaded.\")\n",
    "    sys.exit(1)\n",
    "input_size = x_data_tensors[0].shape[1]\n",
    "if any(t.ndim <= 1 or t.shape[1] != input_size for t in x_data_tensors):\n",
    "     print(\"Error: Input features have inconsistent dimensions or are not 2D across samples.\")\n",
    "     for i, t in enumerate(x_data_tensors):\n",
    "         if t.ndim <= 1 or t.shape[1] != input_size: print(f\"  Problematic sample index: {i}, shape: {t.shape}\")\n",
    "     sys.exit(1)\n",
    "print(f\"  Input feature size: {input_size}\")\n",
    "\n",
    "try:\n",
    "    all_x_concatenated = torch.cat(x_data_tensors, dim=0).numpy()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_x_concatenated)\n",
    "    x_data_normalized_tensors = [torch.tensor(scaler.transform(x.numpy()), dtype=torch.float32) for x in x_data_tensors]\n",
    "    print(\"  Input features normalized (StandardScaler).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during input normalization: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "padded_data = pad_sequence(x_data_normalized_tensors, batch_first=True, padding_value=0.0)\n",
    "print(f\"  Padded data shape: {padded_data.shape}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "num_samples = len(padded_data)\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "train_end_idx = int(num_samples * train_split)\n",
    "val_end_idx = train_end_idx + int(num_samples * val_split)\n",
    "if val_end_idx == train_end_idx and val_end_idx < num_samples: val_end_idx += 1\n",
    "elif val_end_idx >= num_samples: val_end_idx = train_end_idx\n",
    "\n",
    "train_indices = indices[:train_end_idx]\n",
    "val_indices = indices[train_end_idx:val_end_idx]\n",
    "test_indices = indices[val_end_idx:]\n",
    "\n",
    "print(f\"  Train samples: {len(train_indices)}\")\n",
    "print(f\"  Validation samples: {len(val_indices)}\")\n",
    "print(f\"  Test samples: {len(test_indices)}\")\n",
    "\n",
    "if len(train_indices) == 0 or len(test_indices) == 0: print(\"Warning: Training or test set is empty.\")\n",
    "if len(val_indices) == 0: print(\"Warning: Validation set is empty.\")\n",
    "\n",
    "train_dataset = TensorDataset(padded_data[train_indices], y_data[train_indices])\n",
    "val_dataset = TensorDataset(padded_data[val_indices], y_data[val_indices]) if len(val_indices) > 0 else None\n",
    "test_dataset = TensorDataset(padded_data[test_indices], y_data[test_indices])\n",
    "\n",
    "# Use pin_memory=True if using GPU for potentially faster data transfer\n",
    "pin_memory_flag = True if device != torch.device(\"cpu\") else False\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory_flag)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=pin_memory_flag) if val_dataset else None\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=pin_memory_flag)\n",
    "\n",
    "# --- Define Model ---\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        out, h_final = self.gru(x, h)\n",
    "        last_time_step_out = out[:, -1, :]\n",
    "        out = self.fc(last_time_step_out)\n",
    "        return out, h_final\n",
    "\n",
    "# --- Initialize Model, Loss, Optimizer ---\n",
    "print(\"Initializing model...\")\n",
    "model = GRUNet(input_size=input_size, hidden_size=hidden_size,\n",
    "               num_layers=num_layers, output_size=output_size).to(device) # Move model to device\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Move batch data to the device\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred, _ = model(x_batch, None)\n",
    "        loss = criterion(pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), gradient_clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_epoch_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    all_val_outputs = []\n",
    "    all_val_targets = []\n",
    "\n",
    "    if val_loader:\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                # Move batch data to the device\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs, _ = model(x_batch, None)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                epoch_val_loss += loss.item()\n",
    "                all_val_outputs.append(outputs.cpu()) # Collect on CPU\n",
    "                all_val_targets.append(y_batch.cpu()) # Collect on CPU\n",
    "\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_epoch_val_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Val Loss = {avg_epoch_val_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "\n",
    "        # Optional plotting (consider doing it less frequently)\n",
    "        # if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "        #    # ... (plotting code as before) ...\n",
    "\n",
    "    else: # No validation set\n",
    "         epoch_time = time.time() - epoch_start_time\n",
    "         print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "         val_losses.append(None)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# --- Testing ---\n",
    "print(\"\\n--- Starting Testing ---\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_test_outputs = []\n",
    "all_test_targets = []\n",
    "\n",
    "if not test_loader:\n",
    "     print(\"Warning: No test data loaded. Skipping testing.\")\n",
    "else:\n",
    "    test_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            # Move batch data to the device\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs, _ = model(x_batch, None)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            all_test_outputs.append(outputs.cpu()) # Collect on CPU\n",
    "            all_test_targets.append(y_batch.cpu()) # Collect on CPU\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    if len(test_loader) > 0:\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Final Test Loss (MSE): {avg_test_loss:.5f}, Test Time = {test_time:.2f}s\")\n",
    "\n",
    "        all_test_outputs = torch.cat(all_test_outputs, dim=0).numpy()\n",
    "        all_test_targets = torch.cat(all_test_targets, dim=0).numpy()\n",
    "\n",
    "        # --- Plot Test Results ---\n",
    "        print(\"Plotting test results...\")\n",
    "        num_outputs_to_plot = all_test_outputs.shape[1]\n",
    "        fig_test, axes_test = plt.subplots(1, num_outputs_to_plot, figsize=(6 * num_outputs_to_plot, 5.5), squeeze=False)\n",
    "\n",
    "        for i in range(num_outputs_to_plot):\n",
    "            ax = axes_test[0, i]\n",
    "            outputs_i = all_test_outputs[:, i]\n",
    "            targets_i = all_test_targets[:, i]\n",
    "            ax.scatter(outputs_i, targets_i, alpha=0.2)\n",
    "            min_val = min(outputs_i.min() if len(outputs_i) > 0 else 0, targets_i.min() if len(targets_i) > 0 else 0)\n",
    "            max_val = max(outputs_i.max() if len(outputs_i) > 0 else 1, targets_i.max() if len(targets_i) > 0 else 1)\n",
    "            padding = (max_val - min_val) * 0.05\n",
    "            ax.plot([min_val - padding, max_val + padding], [min_val - padding, max_val + padding], 'r--', label='y=x')\n",
    "            ax.set_title(f'Test Set - Output {i+1} (Index {target_indices[i]})')\n",
    "            ax.set_xlabel(\"Predicted Value\")\n",
    "            ax.set_ylabel(\"Actual Value\")\n",
    "            ax.axis('equal')\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Test loader was empty. No test results to plot.\")\n",
    "\n",
    "\n",
    "# --- Plot Training/Validation Loss Curve ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "valid_epochs = [i+1 for i, l in enumerate(val_losses) if l is not None]\n",
    "valid_val_losses = [l for l in val_losses if l is not None]\n",
    "if valid_val_losses:\n",
    "    plt.plot(valid_epochs, valid_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Save Model (Optional) ---\n",
    "# print(f\"\\nSaving model to: {model_save_path}\")\n",
    "# torch.save({\n",
    "#     'epoch': num_epochs,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': avg_test_loss if test_loader and len(test_loader) > 0 else None,\n",
    "#     'input_size': input_size,\n",
    "#     'hidden_size': hidden_size,\n",
    "#     'num_layers': num_layers,\n",
    "#     'output_size': output_size,\n",
    "#     'target_indices': target_indices,\n",
    "#     'scaler_mean': scaler.mean_,\n",
    "#     'scaler_scale': scaler.scale_,\n",
    "#     'y_max_vals': y_max_vals.cpu().numpy()\n",
    "# }, model_save_path)\n",
    "# print(\"Model saved.\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save Model (Optional) ---\n",
    "print(f\"\\nSaving model to: {model_save_path}\")\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_test_loss if test_loader and len(test_loader) > 0 else None, # Save final test loss if available\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "    'output_size': output_size,\n",
    "    'target_indices': target_indices,\n",
    "    'scaler_mean': scaler.mean_, # Save scaler parameters\n",
    "    'scaler_scale': scaler.scale_,\n",
    "    'y_max_vals': y_max_vals.cpu().numpy() # Save target normalization factors\n",
    "}, model_save_path)\n",
    "print(\"Model saved.\")\n",
    "\n",
    "# --- Notify (Optional) ---\n",
    "try:\n",
    "    notify()\n",
    "except NameError:\n",
    "    pass # Ignore if notify function isn't defined\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up device ---\n",
      "Using MPS device (Apple Silicon GPU)\n",
      "Loading data from: data/ppo_baseline_0331_5cost_data\n",
      "Error: Data file not found at data/ppo_baseline_0331_5cost_data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m         loaded_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ppo_baseline_0331_5cost_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Data file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1064\u001b[0m )\n\u001b[1;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lab/lib/python3.11/site-packages/IPython/core/ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse # For plotting covariance\n",
    "import sys # Import sys for sys.exit()\n",
    "import time # For timing epochs\n",
    "from math import pi # For likelihood loss\n",
    "\n",
    "# --- Configuration ---\n",
    "note = 'data' # Corresponds to datanote in your example\n",
    "testnote = 'likelihood_loss_fix' # Added a note for this version\n",
    "agent_name = 'ppo_baseline_0331_5cost'\n",
    "data_path = f'data/{agent_name}_{note}'\n",
    "model_save_path = f'data/{agent_name}_{note}_{testnote}_mps.pt' # Updated save path\n",
    "batch_size = 64\n",
    "num_epochs = 100 # Increased epochs like example\n",
    "learning_rate = 0.01 # Using SGD learning rate from example\n",
    "hidden_size = 32 # Using hidden size from example\n",
    "num_layers = 2\n",
    "gradient_clip_value = 5.0\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "data_subset_size = 1000 # Use subset like example\n",
    "\n",
    "# --- Device Setup (MPS Acceleration) ---\n",
    "print(\"--- Setting up device ---\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "try:\n",
    "    with open(data_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        if not isinstance(loaded_data, (list, tuple)) or len(loaded_data) != 2:\n",
    "             raise TypeError(\"Pickle file should contain a list or tuple of two elements: x_data and ys\")\n",
    "        x_data_raw, ys_raw = loaded_data\n",
    "        if not isinstance(x_data_raw, list) or not isinstance(ys_raw, list):\n",
    "             raise TypeError(\"Both elements in the loaded pickle file should be lists.\")\n",
    "        print(f\"Loaded {len(x_data_raw)} samples.\")\n",
    "\n",
    "        # --- Use Subset ---\n",
    "        if data_subset_size is not None and data_subset_size < len(x_data_raw):\n",
    "            print(f\"Using subset of {data_subset_size} samples.\")\n",
    "            x_data_raw = x_data_raw[:data_subset_size]\n",
    "            ys_raw = ys_raw[:data_subset_size]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_path}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing pickle file: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# 1. Target Variable Processing\n",
    "print(\"Processing target variables (y_data)...\")\n",
    "try:\n",
    "    y_data = [torch.tensor(y, dtype=torch.float32).view(-1) for y in ys_raw]\n",
    "    y_data = torch.stack(y_data)\n",
    "    # --- Select specific columns (indices 3, 9) ---\n",
    "    target_indices = [3, 9]\n",
    "    y_data = y_data[:, target_indices]\n",
    "    ndim = y_data.shape[1] # Number of dimensions for mean prediction (should be 2)\n",
    "    # --- Output size now predicts mean (ndim) + log_std (ndim) ---\n",
    "    output_size = ndim * 2\n",
    "    print(f\"  Target shape: {y_data.shape}\")\n",
    "    print(f\"  ndim (for means): {ndim}\")\n",
    "    print(f\"  Model output size (mean+log_std): {output_size}\")\n",
    "\n",
    "    # --- Removed y_data normalization by max value ---\n",
    "    # y_max_vals, _ = torch.max(y_data, dim=0, keepdim=True)\n",
    "    # y_max_vals[y_max_vals == 0] = 1.0\n",
    "    # y_data = y_data / y_max_vals\n",
    "    # print(\"  Targets normalized by max value per column.\")\n",
    "\n",
    "except IndexError:\n",
    "     print(f\"Error: Target indices {target_indices} out of bounds for loaded y data.\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "     print(f\"Error processing y_data: {e}\")\n",
    "     sys.exit(1)\n",
    "\n",
    "# 2. Input Variable Processing (Normalization & Padding)\n",
    "print(\"Processing input variables (x_data)...\")\n",
    "x_data_tensors = [torch.tensor(x, dtype=torch.float32) for x in x_data_raw]\n",
    "\n",
    "if not x_data_tensors:\n",
    "    print(\"Error: No input data loaded.\")\n",
    "    sys.exit(1)\n",
    "# Check input size consistency before accessing shape[1]\n",
    "if not all(t.ndim > 1 for t in x_data_tensors):\n",
    "     print(\"Error: Some input features are not 2D.\")\n",
    "     # Find problematic shapes\n",
    "     for i, t in enumerate(x_data_tensors):\n",
    "         if t.ndim <= 1 :\n",
    "             print(f\"  Problematic sample index: {i}, shape: {t.shape}\")\n",
    "     sys.exit(1)\n",
    "\n",
    "input_size = x_data_tensors[0].shape[1]\n",
    "if any(t.shape[1] != input_size for t in x_data_tensors):\n",
    "     print(\"Error: Input features have inconsistent dimensions across samples.\")\n",
    "     for i, t in enumerate(x_data_tensors):\n",
    "         if t.shape[1] != input_size: print(f\"  Problematic sample index: {i}, shape: {t.shape}\")\n",
    "     sys.exit(1)\n",
    "print(f\"  Input feature size: {input_size}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    all_x_concatenated = torch.cat(x_data_tensors, dim=0).numpy()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_x_concatenated)\n",
    "    x_data_normalized_tensors = [torch.tensor(scaler.transform(x.numpy()), dtype=torch.float32) for x in x_data_tensors]\n",
    "    print(\"  Input features normalized (StandardScaler).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during input normalization: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "padded_data = pad_sequence(x_data_normalized_tensors, batch_first=True, padding_value=0.0)\n",
    "print(f\"  Padded data shape: {padded_data.shape}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "num_samples = len(padded_data)\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "train_end_idx = int(num_samples * train_split)\n",
    "val_end_idx = train_end_idx + int(num_samples * val_split)\n",
    "if val_end_idx == train_end_idx and val_end_idx < num_samples: val_end_idx += 1\n",
    "elif val_end_idx >= num_samples: val_end_idx = train_end_idx\n",
    "\n",
    "train_indices = indices[:train_end_idx]\n",
    "val_indices = indices[train_end_idx:val_end_idx]\n",
    "test_indices = indices[val_end_idx:]\n",
    "\n",
    "print(f\"  Train samples: {len(train_indices)}\")\n",
    "print(f\"  Validation samples: {len(val_indices)}\")\n",
    "print(f\"  Test samples: {len(test_indices)}\")\n",
    "\n",
    "if len(train_indices) == 0 or len(test_indices) == 0: print(\"Warning: Training or test set is empty.\")\n",
    "if len(val_indices) == 0: print(\"Warning: Validation set is empty.\")\n",
    "\n",
    "train_dataset = TensorDataset(padded_data[train_indices], y_data[train_indices])\n",
    "val_dataset = TensorDataset(padded_data[val_indices], y_data[val_indices]) if len(val_indices) > 0 else None\n",
    "test_dataset = TensorDataset(padded_data[test_indices], y_data[test_indices])\n",
    "\n",
    "pin_memory_flag = True if device != torch.device(\"cpu\") else False\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory_flag)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=pin_memory_flag) if val_dataset else None\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=pin_memory_flag)\n",
    "\n",
    "# --- Define Model ---\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        # Output layer predicts mean (ndim) + log_std (ndim)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        out, h_final = self.gru(x, h)\n",
    "        last_time_step_out = out[:, -1, :]\n",
    "        out = self.fc(last_time_step_out)\n",
    "        return out, h_final\n",
    "\n",
    "# --- Define Custom Loss Function ---\n",
    "def likelihoodloss(pred, target, ndim):\n",
    "    \"\"\"\n",
    "    Calculates negative log-likelihood for a multivariate normal distribution\n",
    "    with diagonal covariance, plus an MSE term on the means.\n",
    "    Assumes pred contains [means (ndim), log_stds (ndim)].\n",
    "    \"\"\"\n",
    "    mus = pred[:, :ndim]\n",
    "    log_stds = pred[:, ndim:] # Predict log standard deviations for stability\n",
    "    stds = torch.exp(log_stds) # Convert back to std dev\n",
    "\n",
    "    # Create diagonal covariance matrices from standard deviations\n",
    "    # Ensure stds are positive before creating distribution\n",
    "    stds = torch.clamp(stds, min=1e-6) # Clamp std devs to avoid issues near zero\n",
    "    scale_tril = torch.diag_embed(stds) # Creates batch of diagonal matrices\n",
    "\n",
    "    # Define the Multivariate Normal distribution\n",
    "    try:\n",
    "        p = torch.distributions.multivariate_normal.MultivariateNormal(mus, scale_tril=scale_tril)\n",
    "        # Calculate log probability, clip for stability\n",
    "        log_prob = torch.clip(p.log_prob(target), -10, 3) # Clipping from example\n",
    "        nll_loss = -torch.mean(log_prob) # Negative log likelihood\n",
    "    except ValueError as e:\n",
    "         print(f\"ValueError in MultivariateNormal or log_prob: {e}\")\n",
    "         print(f\"Mus min/max: {mus.min().item():.2E}/{mus.max().item():.2E}, Stds min/max: {stds.min().item():.2E}/{stds.max().item():.2E}\")\n",
    "         # Return a high loss or handle differently\n",
    "         nll_loss = torch.tensor(10.0, device=pred.device, requires_grad=True) # High loss fallback\n",
    "    except RuntimeError as e:\n",
    "         print(f\"RuntimeError in MultivariateNormal or log_prob: {e}\")\n",
    "         print(f\"Mus shape: {mus.shape}, Scale_tril shape: {scale_tril.shape}, Target shape: {target.shape}\")\n",
    "         # Check for NaNs/Infs\n",
    "         print(f\"Mus finite: {torch.isfinite(mus).all()}, Scale_tril finite: {torch.isfinite(scale_tril).all()}, Target finite: {torch.isfinite(target).all()}\")\n",
    "         nll_loss = torch.tensor(10.0, device=pred.device, requires_grad=True) # High loss fallback\n",
    "\n",
    "\n",
    "    # MSE loss on the means\n",
    "    mseloss = torch.mean((mus - target)**2)\n",
    "\n",
    "    # Combine losses (as in example)\n",
    "    res = nll_loss + mseloss\n",
    "\n",
    "    return res\n",
    "\n",
    "# --- Initialize Model, Loss, Optimizer ---\n",
    "print(\"Initializing model...\")\n",
    "model = GRUNet(input_size=input_size, hidden_size=hidden_size,\n",
    "               num_layers=num_layers, output_size=output_size).to(device)\n",
    "criterion = likelihoodloss # Use the custom loss\n",
    "# --- Use SGD optimizer as in example ---\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Keep Adam as alternative\n",
    "\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    # --- Removed skipping of first epoch ---\n",
    "    # if epoch > 0: # Original code skipped epoch 0\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred, _ = model(x_batch, None)\n",
    "        # Pass ndim to the loss function\n",
    "        loss = criterion(pred, y_batch, ndim=ndim)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # --- Removed retain_graph=True ---\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), gradient_clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Avoid division by zero if train_loader is empty\n",
    "    if len(train_loader) > 0:\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    else:\n",
    "        avg_epoch_train_loss = 0.0\n",
    "    train_losses.append(avg_epoch_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    all_val_outputs = []\n",
    "    all_val_targets = []\n",
    "\n",
    "    if val_loader:\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs, _ = model(x_batch, None)\n",
    "                # Pass ndim to the loss function\n",
    "                loss = criterion(outputs, y_batch, ndim=ndim)\n",
    "                epoch_val_loss += loss.item()\n",
    "                all_val_outputs.append(outputs.cpu())\n",
    "                all_val_targets.append(y_batch.cpu())\n",
    "\n",
    "        if len(val_loader) > 0:\n",
    "            avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "        else:\n",
    "            avg_epoch_val_loss = 0.0\n",
    "        val_losses.append(avg_epoch_val_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Val Loss = {avg_epoch_val_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "\n",
    "        # --- Optional plotting during validation (less frequent) ---\n",
    "        if (epoch + 1) % 20 == 0 or epoch == num_epochs - 1: # Plot every 20 epochs and last\n",
    "             if all_val_outputs:\n",
    "                 all_val_outputs_np = torch.cat(all_val_outputs, dim=0).numpy()\n",
    "                 all_val_targets_np = torch.cat(all_val_targets, dim=0).numpy()\n",
    "                 plt.figure(figsize=(6, 6))\n",
    "                 plt.scatter(all_val_outputs_np[:, 0], all_val_targets_np[:, 0], alpha=0.2)\n",
    "                 min_val = min(all_val_outputs_np[:, 0].min(), all_val_targets_np[:, 0].min())\n",
    "                 max_val = max(all_val_outputs_np[:, 0].max(), all_val_targets_np[:, 0].max())\n",
    "                 plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y=x')\n",
    "                 plt.title(f'Epoch {epoch+1} Validation - Output 1 (Mean)')\n",
    "                 plt.xlabel(\"Predicted Mean\")\n",
    "                 plt.ylabel(\"Actual Value\")\n",
    "                 plt.axis('equal')\n",
    "                 plt.grid(True)\n",
    "                 plt.legend()\n",
    "                 plt.show()\n",
    "\n",
    "    else: # No validation set\n",
    "         epoch_time = time.time() - epoch_start_time\n",
    "         print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {avg_epoch_train_loss:.5f}, Time = {epoch_time:.2f}s\")\n",
    "         val_losses.append(None)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# --- Testing ---\n",
    "print(\"\\n--- Starting Testing ---\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_test_outputs = []\n",
    "all_test_targets = []\n",
    "\n",
    "if not test_loader:\n",
    "     print(\"Warning: No test data loaded. Skipping testing.\")\n",
    "else:\n",
    "    test_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs, _ = model(x_batch, None)\n",
    "            # Pass ndim to the loss function\n",
    "            loss = criterion(outputs, y_batch, ndim=ndim)\n",
    "            test_loss += loss.item()\n",
    "            all_test_outputs.append(outputs.cpu())\n",
    "            all_test_targets.append(y_batch.cpu())\n",
    "    test_time = time.time() - test_start_time\n",
    "\n",
    "    if len(test_loader) > 0 and all_test_outputs:\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Final Test Loss (Custom NLL+MSE): {avg_test_loss:.5f}, Test Time = {test_time:.2f}s\")\n",
    "\n",
    "        all_test_outputs = torch.cat(all_test_outputs, dim=0).numpy()\n",
    "        all_test_targets = torch.cat(all_test_targets, dim=0).numpy()\n",
    "\n",
    "        # --- Plot Test Results ---\n",
    "        print(\"Plotting test results...\")\n",
    "        # Plot predicted means vs actual targets\n",
    "        fig_test_means, axes_test_means = plt.subplots(1, ndim, figsize=(6 * ndim, 5.5), squeeze=False)\n",
    "        fig_test_means.suptitle('Test Set: Predicted Means vs Actual Targets')\n",
    "\n",
    "        for i in range(ndim):\n",
    "            ax = axes_test_means[0, i]\n",
    "            outputs_i = all_test_outputs[:, i] # Predicted mean for dim i\n",
    "            targets_i = all_test_targets[:, i] # Actual target for dim i\n",
    "            ax.scatter(outputs_i, targets_i, alpha=0.2)\n",
    "            min_val = min(outputs_i.min(), targets_i.min())\n",
    "            max_val = max(outputs_i.max(), targets_i.max())\n",
    "            padding = (max_val - min_val) * 0.05\n",
    "            ax.plot([min_val - padding, max_val + padding], [min_val - padding, max_val + padding], 'r--', label='y=x')\n",
    "            ax.set_title(f'Output Dim {i+1} (Index {target_indices[i]})')\n",
    "            ax.set_xlabel(\"Predicted Mean\")\n",
    "            ax.set_ylabel(\"Actual Value\")\n",
    "            ax.axis('equal')\n",
    "            ax.grid(True)\n",
    "            ax.legend()\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "        # --- Plot Predicted Covariance Ellipses ---\n",
    "        print(\"Plotting predicted covariance ellipses...\")\n",
    "        fig_test_cov, ax_test_cov = plt.subplots(figsize=(8, 8))\n",
    "        ax_test_cov.scatter(all_test_targets[:, 0], all_test_targets[:, 1], alpha=0.5, label='Actual Targets', s=10)\n",
    "\n",
    "        # Function to plot ellipse (requires matplotlib.patches)\n",
    "        def plot_cov_ellipse(cov, pos, nstd=1, ax=None, **kwargs):\n",
    "            \"\"\"Plots an nstd sigma error ellipse based on the specified covariance matrix.\"\"\"\n",
    "            ax = ax or plt.gca()\n",
    "            # Ensure covariance matrix is symmetric positive semi-definite\n",
    "            cov = (cov + cov.T) / 2 # Ensure symmetry\n",
    "            eigvals, eigvecs = np.linalg.eigh(cov) # Use eigh for symmetric matrices\n",
    "            eigvals = np.maximum(eigvals, 0) # Ensure non-negative eigenvalues\n",
    "\n",
    "            order = eigvals.argsort()[::-1]\n",
    "            eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "\n",
    "            vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n",
    "            theta = np.degrees(np.arctan2(vy, vx))\n",
    "\n",
    "            width, height = 2 * nstd * np.sqrt(eigvals)\n",
    "            ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n",
    "            ax.add_artist(ellip)\n",
    "            return ellip\n",
    "\n",
    "        # Extract means and calculate covariance matrices\n",
    "        pred_means = all_test_outputs[:, :ndim]\n",
    "        pred_log_stds = all_test_outputs[:, ndim:]\n",
    "        pred_stds = np.exp(pred_log_stds)\n",
    "        # Plot subset of ellipses for clarity\n",
    "        num_ellipses_to_plot = min(100, len(pred_means))\n",
    "        indices_to_plot = np.random.choice(len(pred_means), num_ellipses_to_plot, replace=False)\n",
    "\n",
    "        for idx in indices_to_plot:\n",
    "            mu = pred_means[idx]\n",
    "            std = pred_stds[idx]\n",
    "            # Create diagonal covariance matrix\n",
    "            cov = np.diag(np.maximum(std, 1e-6)**2) # Use std^2, ensure positive variance\n",
    "            plot_cov_ellipse(cov, mu, nstd=1, ax=ax_test_cov, alpha=0.1, color='red') # Plot 1-std ellipse\n",
    "\n",
    "        ax_test_cov.set_title(f'Test Set: Actual Targets and Predicted 1-std Ellipses (Subset)')\n",
    "        ax_test_cov.set_xlabel(f'Target Dim 1 (Index {target_indices[0]})')\n",
    "        ax_test_cov.set_ylabel(f'Target Dim 2 (Index {target_indices[1]})')\n",
    "        ax_test_cov.axis('equal')\n",
    "        ax_test_cov.grid(True)\n",
    "        # ax_test_cov.legend() # Legend might get cluttered with ellipses\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Test loader was empty or no results collected. No test results to plot.\")\n",
    "\n",
    "\n",
    "# --- Plot Training/Validation Loss Curve ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "valid_epochs = [i+1 for i, l in enumerate(val_losses) if l is not None]\n",
    "valid_val_losses = [l for l in val_losses if l is not None]\n",
    "if valid_val_losses:\n",
    "    plt.plot(valid_epochs, valid_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Custom Loss (NLL + MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Save Model (Optional) ---\n",
    "# print(f\"\\nSaving model to: {model_save_path}\")\n",
    "# torch.save({\n",
    "#     'epoch': num_epochs,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'loss': avg_test_loss if test_loader and len(test_loader) > 0 and 'avg_test_loss' in locals() else None,\n",
    "#     'input_size': input_size,\n",
    "#     'hidden_size': hidden_size,\n",
    "#     'num_layers': num_layers,\n",
    "#     'output_size': output_size, # This is mean+log_std size\n",
    "#     'ndim': ndim, # Store the dimension of the mean/target vector\n",
    "#     'target_indices': target_indices,\n",
    "#     'scaler_mean': scaler.mean_,\n",
    "#     'scaler_scale': scaler.scale_,\n",
    "#     # 'y_max_vals': y_max_vals.cpu().numpy() # Removed y normalization\n",
    "# }, model_save_path)\n",
    "# print(\"Model saved.\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
